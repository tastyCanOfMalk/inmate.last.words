---
title: 'Last words of Texas death row inmates'
subtitle: 'An exploratory text analysis'
author: 'Edward Yu'
date: '2019-03-23'
output:
  html_document:
      theme: flatly
      highlight: tango
      toc: true
      toc_float:
        collapsed: true
      toc_depth: 3
      df_print: paged
      code_folding: show
      # fig_width: 7
      # fig_height: 6
      # fig_caption: true
      citation_package: natbib
bibliography: bib.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo    = TRUE,
                      error   = FALSE,
                      message = FALSE,
                      warning = FALSE)

if(!require(tidyverse)) install.packages("tidyverse")
library(tidyverse)
if(!require(naniar)) install.packages("naniar")
library(naniar) # gg_miss
if(!require(mice)) install.packages("mice")
library(mice) # gg_miss
if(!require(ggridges)) install.packages("ggridges")
library(ggridges)
if(!require(viridis)) install.packages("viridis")
library(viridis)
if(!require(gridExtra)) install.packages("gridExtra")
library(gridExtra)
if(!require(ggalt)) install.packages("ggalt")
library(ggalt)
if(!require(tidytext)) install.packages("tidytext")
library(tidytext)
if(!require(RColorBrewer)) install.packages("RColorBrewer")
library(RColorBrewer)
if(!require(wordcloud)) install.packages("wordcloud")
library(wordcloud)
if(!require(reshape2)) install.packages("reshape2")
library(reshape2)

# clear history
rm(list = ls())

# setwd("I:/Code-CAD/R/inmate.last.words")
setwd("D:/Code/R/inmate.last.words")
```

# Introduction
I found this data via a Kaggle post: https://www.kaggle.com/mykhe1097/last-words-of-death-row-inmates. The original data was taken from: https://www.tdcj.texas.gov/death_row/dr_executed_offenders.html.  

The title alone was interesting enough to merit more than a passing glance. I'll start with a very simple, generalized analysis, moving on to a more in depth sentiment analysis utilizing the tidytext package.

# Import
In general, my initial process is very similar from project to project: import the data, use `glimpse` and `summary` to get an idea of the structure, sort all variables by descending unique levels, and plot the missingness of each variable. I find that even if this information isn't specifically useful in every case it gives a nice overview of the data.  

```{r}
# import
x <- read_csv("data/Texas Last Statement - CSV.csv")

glimpse(x)
summary(x)

# Find unique levels
x.levels <- cbind(colnames(x),
                  (as.data.frame(sapply(x,function(x) length(unique(x))))))
colnames(x.levels) <- c("var","levels")
row.names(x.levels) <- NULL
x.levels[order(-x.levels[,2]),]

# % missingness using naniar package
gg_miss_var(x, show_pct = T)
```

At this point I can already tell that I'm going to drop the more specific `HispanicVictim` or `BlackVictim` variables in favor of the more generalized `NumberVictim` variable. The `AgeWhenReceived` variable also sounds like it could be an interesting variable to work with paired with `Race` or `Age`. 

# Impute
I'll start by listing all the variables I might be interested in working with.  

  1. First name
  2. Last name
  3. Race
  4. Education level
  5. Previous crime commited?
  6. Number of victims
  7. Number of female victims
  8. Number of male victims
  9. Age when received
  10. Age executed
  11. Codefendants
  12. Last statement

Generally we only impute if 5% or less of the data is missing. In this case, `EducationLevel`, `PreviousCrime`, `Codefendants`, are all missing more than 5% of their data. In this case, and for the sake of exploration, I will bend the rule and attempt imputation regardless.  

My first analyses used simple median or mean values to replace NA's. This time around I've become interested in a package called `mice`, which stands for Multivariate Imputation by Chained Equations. As my familiarity with the package is limited at this time, I followed the following reference quite closely: https://www.r-bloggers.com/imputing-missing-data-with-r-mice-package/.  

The visualizations that follow show the imputed (red) values falling well within actual (blue) values, indicating the imputation was reasonable.  

```{r results='hide'}
# rename columns, select columns of interest
x1 <- x %>% 
  mutate(AWR = AgeWhenReceived) %>% 
  mutate(AgeExec = Age) %>% 
  select(FirstName, 
         LastName, 
         Race, 
         EducationLevel,
         PreviousCrime, 
         NumberVictim, 
         FemaleVictim, 
         MaleVictim, 
         AWR,
         AgeExec, 
         Codefendants, 
         LastStatement)

# remove columns with characters for stripplot() visualization
rem <- c(1,2,3,12)

# imputation process using pmm, 20 iterations
tempData <- mice(x1[-rem],
                 m=1,
                 maxit=20,
                 meth='pmm',
                 seed=1)

# compare distribution of actual values with imputed values; red=imputed
xyplot(tempData,
       AWR~
         PreviousCrime+
         EducationLevel+
         FemaleVictim+
         MaleVictim+
         NumberVictim+
         Codefendants,
       pch=18,
       cex=1)

# can also compare using density plot; red=imputed
densityplot(tempData)

# and stripplot
stripplot(tempData,pch=20,cex=1)

# rerun imputation on complete dataset and compare to original
tempData <- mice(x1,m=1,maxit=50,meth='pmm',seed=1)

# replace missing values with imputed values using first dataset
xCom <- complete(tempData,1)
```

Checking the `summary` data of the original dataset and comparing to the new, imputed dataset confirms that our imputed data falls within the distribution of our original data.  

```{r}
# comparing the completed data to the original again shows imputed data makes sense
summary(xCom)[,-rem]
summary(x1)[,-rem]
```

# Fix
**Time served measurement**
I wanted to add a time served, or `Served` measurement. Upon doing this an error in the data presented itself plainly in the `summary` measure which reporeted a value of -1.  

```{r}
# add time served column
xCom <- xCom %>% 
  mutate(Served = AgeExec-AWR)

# have an odd -1 years served
summary(xCom$Served)

# find datapoint
which.min(xCom$Served)
xCom[266,]
```

If we track this value down we can conclude that either the Age when received or the Age when executed measures are incorrect. Since the median value of our newly created time served variable is equal to 10, it's safe to assume that either our `AWR` should be 27, rather than 37; or `AgeExec` should be 46, rather than 36.  

It turns out that a simple internet search of the inmate's name leads us to the actual age of execution being 47 rather than 37: http://www.clarkprosecutor.org/html/death/US/walker796.htm.

```{r}
# change AgeExec and Served for 266
xCom[266,]$AgeExec <- 47
xCom[266,]$Served <- 10
```

# EDA
We're ready to do some exploratory data analysis at this point. What I'm interested in are probably cliche points of analysis but given the variables presented they seem to stand out.  

  * Race vs Time served
  * Race vs Age when received

## Race vs time served
Does race play a part in how many years are served leading up to the execution date? Based on summary data it seems that time served is roughly the same for all races. We can plot density and histogram plots to show the relative and absolute differences in time served between races.  

Note there were 2 races classified as `Other`, which were removed.  

```{r}
# check races, remove 'other' when plotting
summary(as.factor(x$Race))

xCom %>% 
  filter(Race != "Other") %>% 
  group_by(Race) %>% 
  summarise(median(Served))

g1 <- xCom %>% 
  filter(Race != "Other") %>% 
  ggplot(aes(x=Served,fill=Race))+
  geom_density(alpha=.8)+
  ggtitle("Time served by race (relative)")

g2 <- xCom %>% 
  filter(Race != "Other") %>% 
  ggplot(aes(x=Served,fill=Race))+
  geom_histogram()+
  facet_wrap(Race~.)+
  ggtitle("Time served by race (absolute)")

grid.arrange(g1,g2,nrow=1)
```

We see clearly, based on the density plot, that time served between the different races represented are very similar. When we look at the histogram we do notice there are far fewer hispanics and appear to be slightly more whites in this population.

## Race vs age when received/executed
What can we discover about when the inmates were first received and how race impacts this?  Along the same vein, does age of execution remain similar between races?  

```{r}
# Age received, executed, served by race
xCom %>% 
  filter(Race != "Other") %>% 
  group_by(Race) %>% 
  summarise(AWR=median(AWR),
            AgeExec=median(AgeExec),
            Served=median(Served))

# dumbell summary
df.db <- xCom %>% 
  filter(Race != 'Other') %>% 
  group_by(Race) %>% 
  summarise(AWR=median(AWR),
            AgeExec=median(AgeExec),
            Served=median(Served))

df.db %>% 
  ggplot()+
  # geom_segment(aes(y=Race,yend=Race,x=0,xend=1),color="Red",size=.15)+
  geom_dumbbell(aes(y=Race, x=AWR, xend=AgeExec),
                size=5, 
                color="#e3e2e1",
                colour_x = "#5b8124", 
                colour_xend = "#bad744")+
  xlab("Age")+
  theme_bw()

# detailed boxplots of all stats
df <- xCom %>% 
  filter(Race != "Other") %>% 
  select(Race,AWR,AgeExec,Served)

df <- df %>% 
  gather(key = "Age.thingy", value = "Years", colnames(df)[2:4]) %>%
  mutate(Race = factor(Race))

ggplot(data = df, aes(x = Race, y = Years, fill = Race))+
  geom_boxplot(outlier.shape=NA)+
  geom_jitter(shape=16,position=position_jitter(.15),alpha=.3)+
  coord_flip()+
  facet_wrap(~Age.thingy, nrow = 3, strip.position = "top")+
  theme_bw()
```

We see that Blacks tend to be received sooner than Hispanics, while Whites are generally received much later than both other Races. Time served before execution does remain quite similar between all races.  

## Number of victims vs
Does anything change when the number of victims increases? It's a bit difficult to make concrete conclusions based on such little data. We can see pretty quickly that most inmates had a single victim. Time served does not seem to fluctuate much based solely on the number of victims: the median seems to stay within the 10 year range we saw before.

```{r}
# does time served change as number of victims change?
p3 <- xCom %>% 
  # filter(NumberVictim > 0 & NumberVictim < 6) %>% 
  ggplot(aes(x=Served,fill=as.factor(NumberVictim)))+
  geom_histogram(stat="count")+
  # facet_wrap(NumberVictim~.,nrow=3)+
  ggtitle("Histogram of years served, fill = NumberVictim")+
  theme(legend.position = "none")+
  scale_fill_brewer(type="qual",palette=3)

# try boxplot instead with numbervictims = box
p4 <- xCom %>% 
  as_tibble() %>% 
  select(Served, NumberVictim) %>% 
  ggplot(aes(y=Served,x=as.factor(NumberVictim)))+
  geom_boxplot(outlier.color = NA)+
  coord_flip()+
  geom_jitter(width=.2,alpha=.4,aes(color=as.factor(NumberVictim)))+
  theme(legend.position = "none")+
  ggtitle("Number of victims vs Served")+
  scale_color_brewer(type="qual",palette=3)+
  xlab("NumberVictim")

grid.arrange(p3,p4,nrow=1)

```

Education level is also relatively constant throughout the range of inmates. In the standard histogram we see a huge dropoff after education level 12 (assumedly High School, grade 12). In fact, our largest peaks occur between grades 9-12, indicating that not completing high school has a potentially significant impact on becoming a deathrow inmate.  

When comparing the number of victims to education level, however, we do not see much of a relationship.  

```{r}
# education level vs number of victims
e1 <- xCom %>% 
  # filter(NumberVictim > 0 & NumberVictim < 6) %>%
  ggplot(aes(x=EducationLevel,fill=as.factor(NumberVictim)))+
  geom_histogram(stat="count")+
  # facet_wrap(NumberVictim~.,nrow=2)+
  ggtitle("Histogram of education level, fill = NumberVictim")+
  scale_fill_brewer(type="qual",palette=3)+
  theme(legend.position = "none")

e2 <- xCom %>% 
  # as_tibble() %>% 
  # select(Served, NumberVictim) %>% 
  ggplot(aes(y=EducationLevel,x=as.factor(NumberVictim)))+
  geom_boxplot(outlier.color = NA)+
  coord_flip()+
  geom_jitter(width=.2,alpha=.4,aes(color=as.factor(NumberVictim)))+
  theme(legend.position = "none")+
  ggtitle("Number of victims vs Served")+
  scale_color_brewer(type="qual",palette=3)+
  xlab("NumberVictim")

grid.arrange(e1,e2,nrow=1)
```

# Text analysis
We garnered a few minor insights from looking at the data about potential differences among the inmates. Now we take a turn and see what they all might have in common regarding their final words.  

## Common words
Perhaps the easiest analysis is to simply list the most popular words used. Before doing so we need to add a few stopwords that appear too frequently and tidy the data for use in our plots.

```{r}
y <- x$LastStatement
y <- tibble(y)

custom_stop_words <- 
  bind_rows(stop_words,
            data_frame(word = c("ya'll", 
                                "y'all",
                                "warden"),
                       lexicon ="custom"))

# tidy the text
y.tidy <- y %>% 
  unnest_tokens(word,y) %>% 
  anti_join(custom_stop_words)

# list of top 20 used words, sorted
y.tidy %>% 
  count(word, sort = TRUE) %>% 
  slice(1:20)

# plot of above list of top used words
y.tidy %>% 
  count(word, sort = TRUE) %>% 
  filter(n>80) %>% 
  mutate(word=reorder(word,n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()+
  theme_bw()

# wordcloud of above list of top used words
pal <- brewer.pal(8,"Dark2")
y.tidy %>% 
  count(word) %>% 
  with(wordcloud(word,n,
                 max.words=80,
                 min.freq=20,
                 colors=pal,
                 random.order = F,
                 random.color = F,
                 scale=c(3,.5)))

# Comparison wordcloud of top used words according to (bing) sentiment
## negative words likely taken grossly out of context in this case
y.tidy %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(max.words = 80
                   # scale=c(3,.2)
                   )
```

We've displayed the most popular words in a few different ways. 
The top word used by far is "love". We see "family" and "forgive" and "peace" show up quite frequently as well which may indicate that most last statements are expressing a need for spiritual forgiveness, or love to families, or peace in death. When we sort by negative and positive sentiment we also get an idea of a potential bitterness in some of the last statements, though it's highly possible that many of the words are mischaracterized via use of negating prefixes.  

## Term frequency by Race
Are there differences in words used when we segregate our last statements by race?  

When we unnest our words and group them by race we can see that total words used per race follows our expectations based on number of inmates (Hispanic < Black < White).  

Term frequency refers to how frequently an item appears in a document. Because unimportant words like 'I', 'you', 'to', etc., may appear too frequently to garner any use from this measure. The plot below illustrates this.  

```{r}
# select statement and race columns
x.1 <- x %>% 
  select(Race, LastStatement)

# unnested words used per race
x.2 <- x.1 %>% 
  unnest_tokens(word,LastStatement) %>% 
  count(Race,word,sort=T) %>% 
  ungroup()

# total words used per race
x.3 <- x.2 %>% 
  group_by(Race) %>% 
  summarise(total=sum(n))
x.3

# each word used compared to total used per race
# common words like i,you, to, etc appear very frequently
x.4 <- left_join(x.2,x.3)
x.4

# graphical representation of the above word distribution
ggplot(x.4,aes(n/total,fill=Race))+
  geom_histogram(show.legend=FALSE)+
  xlim(NA,0.009)+
  facet_wrap(~Race,scales="free_y")
```

Instead of having to manually add all these common words as stop words, we can use a terms inverse document frequency (idf) instead. In this case we use the `tf-idf` measure to find the important words in the document that are common, but not *too* common [@tidy].

```{r}
x.6 <- x.4 %>% 
  bind_tf_idf(word,Race,n)

x.6 %>% 
  filter(Race != "Other") %>% 
  arrange(desc(tf_idf)) %>% 
  mutate(word=factor(word,levels=rev(unique(word)))) %>% 
  group_by(Race) %>% 
  top_n(15,tf_idf) %>% 
  ungroup() %>% 
  ggplot(aes(word, tf_idf,fill=Race))+
  geom_col(show.legend=F)+
  labs(x=NULL,y="tf-df")+
  facet_wrap(~Race,scale="free")+
  coord_flip()
```

# Bigrams
Our sentiment analysis showed a few negative words that may have falsely represented negative sentiment due to a modifier prefix (*e.g.* not guilty, no pain; never loved, etc.). We can attempt to better understand the relationships between words by taking apart our last statements in pairs of words, or bigrams.  

```{r}
# collect bigrams
x.bigrams <- x.1 %>% 
  unnest_tokens(bigram,LastStatement,token="ngrams",n=2)

# most common bigrams
x.bigrams %>% 
  count(bigram,sort=T)

# remove stop words occurring in either word pair
x.bigrams.separated <- x.bigrams %>% 
  separate(bigram, c("word1","word2"),sep=" ")

x.bigrams.filtered <- x.bigrams.separated %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)

# new most common bigrams
x.bigram.counts <- x.bigrams.filtered %>% 
  count(word1,word2,sort=T)
x.bigram.counts
```

And we can throw a wordcloud of common bigrams for all inmates as well as filtered by race.  

## Wordcloud of bigrams {.tabset}

### All
```{r}
# recombine bigrams without stop words
x.bigrams.united <- x.bigrams.filtered %>% 
  unite(bigram,word1,word2,sep=" ") 

# wordcloud of most common bigrams ALL
bg.all <- x.bigrams.united %>% 
  count(bigram) %>% 
  with(wordcloud(bigram,n,
                 max.words=80,
                 min.freq=5,
                 colors=pal,
                 random.order = F,
                 random.color = F,
                 scale=c(3,.5)))
```

### White
```{r}
# wordcloud of most common bigrams WHITE
bg.white <- x.bigrams.united %>%
  filter(Race == "White") %>% 
  count(bigram) %>% 
  with(wordcloud(bigram,n,
                 max.words=80,
                 min.freq=3,
                 colors=pal,
                 random.order = F,
                 random.color = F,
                 scale=c(3,.5)))
```

### Black
```{r}
# wordcloud of most common bigrams BLACK
bg.black <- x.bigrams.united %>%
  filter(Race == "Black") %>% 
  count(bigram) %>% 
  with(wordcloud(bigram,n,
                 max.words=80,
                 min.freq=3,
                 colors=pal,
                 random.order = F,
                 random.color = F,
                 scale=c(3,.5)))
```

### Hispanic
```{r}
# wordcloud of most common bigrams HISPANIC
bg.hispanic <- x.bigrams.united %>%
  filter(Race == "Hispanic") %>% 
  count(bigram) %>% 
  with(wordcloud(bigram,n,
                 max.words=80,
                 min.freq=3,
                 colors=pal,
                 random.order = F,
                 random.color = F,
                 scale=c(3,.5))) 

```






```{r}
x.bigrams.united <- x.bigrams.filtered %>% 
  unite(bigram,word1,word2,sep=" ")

x.bigrams.filtered %>% 
  filter(word1 == "polunsky") %>% 
  count(Race, word1,sort=T)

x.bigram.tf.idf <- x.bigrams.united %>% 
  count(Race,bigram) %>% 
  bind_tf_idf(bigram,Race,n) %>% 
  arrange(desc(tf_idf))

x.bigram.tf.idf %>% 
  filter(Race!="Other") %>% 
  arrange(desc(tf_idf)) %>%
  mutate(bigram=factor(bigram,levels=rev(unique(bigram)))) %>%
  group_by(Race) %>% 
  top_n(5) %>% 
  ungroup() %>%
  ggplot(aes(bigram, tf_idf,fill=Race))+
  geom_col(show.legend=F)+
  labs(x=NULL,y="tf-df")+
  facet_wrap(~Race,scale="free")+
  coord_flip()

```

